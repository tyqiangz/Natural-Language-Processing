{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation\n",
    "\n",
    "Mathematically, LDA assumes the following generative process for each document $\\boldsymbol{w}$ in a corpus $D$:\n",
    "\n",
    "1. Choose $N \\sim$ Poisson$(\\xi)$. $N$ represents the no. of words for the new document.\n",
    "2. Choose $\\theta \\sim \\operatorname{Dir}(\\alpha)$.\n",
    "3. For each of the $N$ words $w_{n}$\n",
    "    1. Choose a topic $z_{n} \\sim$ Multinomial$(\\theta)$.\n",
    "    2. Choose a word $w_{n}$ from $p\\left(w_{n} | z_{n}, \\beta\\right)$, a multinomial probability conditioned on the topic $z_{n}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate a corpus\n",
    "rawdocs = ['eat turkey on turkey day holiday',\n",
    "          'i like to eat cake on holiday',\n",
    "          'turkey trot race on thanksgiving holiday',\n",
    "          'snail race the turtle',\n",
    "          'time travel space race',\n",
    "          'movie on thanksgiving',\n",
    "          'movie at air and space museum is cool movie',\n",
    "          'aspiring movie star']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['eat', 'turkey', 'on', 'turkey', 'day', 'holiday'],\n",
       " ['i', 'like', 'to', 'eat', 'cake', 'on', 'holiday'],\n",
       " ['turkey', 'trot', 'race', 'on', 'thanksgiving', 'holiday'],\n",
       " ['snail', 'race', 'the', 'turtle'],\n",
       " ['time', 'travel', 'space', 'race'],\n",
       " ['movie', 'on', 'thanksgiving'],\n",
       " ['movie', 'at', 'air', 'and', 'space', 'museum', 'is', 'cool', 'movie'],\n",
       " ['aspiring', 'movie', 'star']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = [rawdoc.split(' ') for rawdoc in rawdocs]\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "K = 2 # number of topics\n",
    "ALPHA = 1 # hyperparameter. single value indicates symmetric dirichlet prior. higher=>scatters document clusters\n",
    "ETA = 0.001 # hyperparameter\n",
    "ITERATIONS = 3 # iterations for collapsed gibbs sampling.  This should be a lot higher than 3 in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'aspiring', 'holiday', 'turtle', 'star', 'time', 'movie', 'trot', 'turkey', 'air', 'like', 'eat', 'day', 'travel', 'snail', 'at', 'to', 'cool', 'the', 'cake', 'i', 'is', 'and', 'on', 'thanksgiving', 'space', 'museum', 'race'}\n"
     ]
    }
   ],
   "source": [
    "## Assign WordIDs to each unique word\n",
    "unique_words = set()\n",
    "\n",
    "for doc in docs:\n",
    "    for word in doc:\n",
    "        unique_words.add(word)\n",
    "        \n",
    "print(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'aspiring': 0, 'holiday': 1, 'turtle': 2, 'star': 3, 'time': 4, 'movie': 5, 'trot': 6, 'turkey': 7, 'air': 8, 'like': 9, 'eat': 10, 'day': 11, 'travel': 12, 'snail': 13, 'at': 14, 'to': 15, 'cool': 16, 'the': 17, 'cake': 18, 'i': 19, 'is': 20, 'and': 21, 'on': 22, 'thanksgiving': 23, 'space': 24, 'museum': 25, 'race': 26}\n"
     ]
    }
   ],
   "source": [
    "vocab = {}\n",
    "\n",
    "index = 0\n",
    "\n",
    "for word in unique_words:\n",
    "    vocab[word] = index\n",
    "    index += 1\n",
    "    \n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[10, 7, 22, 7, 11, 1],\n",
       " [19, 9, 15, 10, 18, 22, 1],\n",
       " [7, 6, 26, 22, 23, 1],\n",
       " [13, 26, 17, 2],\n",
       " [4, 12, 24, 26],\n",
       " [5, 22, 23],\n",
       " [5, 14, 8, 21, 24, 25, 20, 16, 5],\n",
       " [0, 5, 3]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Replace words in documents with wordIDs\n",
    "indexed_doc = []\n",
    "\n",
    "for doc in docs:\n",
    "    indexed_doc.append([vocab[word] for word in doc])\n",
    "    \n",
    "indexed_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word-topic matrix \n",
    "wt = np.zeros(shape=(K, len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @ta : topic assignment list\n",
    "ta = [np.zeros(len(doc)) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0., 0., 0., 0., 0., 0.]),\n",
       " array([0., 0., 0., 0., 0., 0., 0.]),\n",
       " array([0., 0., 0., 0., 0., 0.]),\n",
       " array([0., 0., 0., 0.]),\n",
       " array([0., 0., 0., 0.]),\n",
       " array([0., 0., 0.]),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " array([0., 0., 0.])]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @dt : counts correspond to the number of words assigned to each topic for each document\n",
    "dt = np.zeros(shape=(len(docs),K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
